{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-training pruning: iterative magnitude pruning (IMP)\n",
    "\n",
    "## When iteration is one, IMP downgrades to one-shot magnitude pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "from models import *\n",
    "from utils.utils import progress_bar, train, test\n",
    "from utils.pruner import pruning_model_random, check_sparsity, pruning_model, prune_model_custom, extract_mask, remove_prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 2 # how many epochs for each training period?\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# 1 if OMP is applied. However, to get a fully retrained OMP model, pruning_times should be set to 2. \n",
    "# The model before the last pruning step is the fully trained OMP model given the desired pruning ratio.\n",
    "pruning_times = 2\n",
    "pruning_ratio = 0.2\n",
    "rewinding_epoch = 1\n",
    "\n",
    "save_dir = 'checkpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building model..\n",
      "######################################## Start Standard Training Iterative Pruning ########################################\n",
      "******************************************\n",
      "pruning state 0\n",
      "******************************************\n",
      "* remain weight =  100.0 %\n",
      "\n",
      "Epoch: 0\n",
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.965 | Acc: 30.282% (15141/5000 391/391 \n",
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.668 | Acc: 40.640% (4064/1000 100/100 ==========>............]  Step: 0ms | Tot: 0ms | Loss: 1.668 | Acc: 41.020% (2051/500 50/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.422 | Acc: 47.748% (23874/5000 391/391 \n",
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.339 | Acc: 51.820% (5182/1000 100/100 \n",
      "Saving..\n",
      "start unstructured pruning\n",
      "remove pruning\n",
      "start unstructured pruning with custom mask\n",
      "******************************************\n",
      "pruning state 1\n",
      "******************************************\n",
      "* remain weight =  80.00000358447606 %\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/yaoyugua/miniconda3/envs/pruning/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.384 | Acc: 49.512% (24756/5000 391/391 \n",
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.254 | Acc: 53.620% (5362/1000 100/100 =======>...............]  Step: 0ms | Tot: 0ms | Loss: 1.248 | Acc: 53.568% (1982/370 37/100 =================>.......]  Step: 0ms | Tot: 0ms | Loss: 1.258 | Acc: 53.639% (3862/720 72/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.235 | Acc: 55.338% (27669/5000 391/391 \n",
      " [========================>]  Step: 0ms | Tot: 0ms | Loss: 1.224 | Acc: 55.330% (5533/1000 100/100 =======>...............]  Step: 0ms | Tot: 0ms | Loss: 1.216 | Acc: 55.462% (2163/390 39/100 \n",
      "Saving..\n",
      "start unstructured pruning\n",
      "remove pruning\n",
      "start unstructured pruning with custom mask\n",
      "* remain weight =  64.00000465981887 %\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "print('######################################## Start Standard Training Iterative Pruning ########################################')\n",
    "\n",
    "for state in range(pruning_times):\n",
    "\n",
    "    print('******************************************')\n",
    "    print('pruning state', state)\n",
    "    print('******************************************')\n",
    "\n",
    "    check_sparsity(net)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch+epochs):\n",
    "        train(epoch, device, net, trainloader, optimizer, criterion)\n",
    "        if state == 0:\n",
    "            if (epoch+1) == rewinding_epoch:\n",
    "                torch.save(net.state_dict(), os.path.join(save_dir, 'epoch_{}_rewind_weight.pt'.format(epoch+1)))\n",
    "                rewind_init = deepcopy(net.state_dict())\n",
    "        test(epoch, device, net, testloader, criterion, best_acc)\n",
    "        scheduler.step()\n",
    "    \n",
    "    # model pruning and rewinding\n",
    "    pruning_model(net, pruning_ratio)\n",
    "    current_mask = extract_mask(net.state_dict())\n",
    "    remove_prune(net)\n",
    "\n",
    "    net.load_state_dict(rewind_init, strict=True)\n",
    "    prune_model_custom(net, current_mask)\n",
    "    \n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for _ in range(rewinding_epoch):\n",
    "        scheduler.step()\n",
    "    \n",
    "check_sparsity(net)\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d54af12f29f52c9f4f573a3d075b10e3d7001a1506ff745cf3cdc9d4445168e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
